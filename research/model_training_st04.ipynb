{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Wine.Utils import create_directory,read_yaml,download_data_from_s3,save_object\n",
    "from src.Wine.loggers import logger\n",
    "from src.Wine.Exception import CustomException\n",
    "from src.Wine.Constants import *\n",
    "import os,sys\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for training the model we gonna used hyperparameter tuning technique\n",
    "#(known as grid search cv-->will used all the parameter for training it reqired more time to compute the parameter\n",
    "#that we have defined in yaml and identify which parameter will good for this dataset\n",
    "#or random search cv randomly used the parameter to training purpose comparatively faster than grid)\n",
    "#in which i am gonna used 4 algorithm for this dataset\n",
    "#elasticnet,random forest regressor,support vector regressor,decision tree regressor,boosting regressor\n",
    "\n",
    "#calling the algorithm class\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "import pandas as pd,numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 3) update the entity file:- entity file is nothing but whatever parameter we have used in yaml file \n",
    "#we gonna defined them as a class variable\n",
    "@dataclass\n",
    "class ModelTrainingConfig():\n",
    "    #defining the class variable\n",
    "    root_dir_path:Path\n",
    "    save_best_model_dirpath:Path\n",
    "    all_param:dict\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig():\n",
    "    #defining class variable along with dtypes\n",
    "    root_dir_path:Path\n",
    "    save_obj_dirpath: Path\n",
    "    csv_dir_path: Path\n",
    "    target_column:dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step4)update the configuration manager file in this file we read yaml file and create directory \n",
    "#and creating object of class variable and assigning value to \n",
    "#the class variable parameter and taking rtn as fuctn\n",
    "class ConfigurationManager():\n",
    "    #creating constructor to initialize the instance variable\n",
    "    def __init__(self,config_filepath = CONFIG_FILEPATH,param_filepath=PARAM_FILEPATH,schema_filepath=SCHEMA_FILEPATH):\n",
    "        self.config = read_yaml(config_filepath) #this will rtn the value as configbox dict\n",
    "        self.param = read_yaml(param_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        #creating artifact directory in project structure\n",
    "        create_directory([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_data_transformation_config(self):\n",
    "        #creating local variable which was used inside this method\n",
    "        transform = self.config.data_transformation\n",
    "        target_coln = self.schema\n",
    "\n",
    "        #creating root directory in artifacts folder for datatransformation\n",
    "        create_directory([transform.root_dir_path]) #create artifacts/data_transformation folder\n",
    "\n",
    "        #creating an object &\n",
    "        #assigining the value to DataTransformationConfig class variable and taking rtn as function\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir_path=transform.root_dir_path,\n",
    "            save_obj_dirpath=transform.save_obj_dirpath,\n",
    "            csv_dir_path=transform.csv_dir_path,\n",
    "            target_column=target_coln.target_column\n",
    "        )\n",
    "\n",
    "        return data_transformation_config\n",
    "    \n",
    "    \n",
    "\n",
    "    #another method we used to get model training config!!!\n",
    "    def get_model_training_config(self) ->ModelTrainingConfig:\n",
    "        #initializing the local variable which is used inside this method only\n",
    "        config = self.config.model_training\n",
    "        param = self.param\n",
    "\n",
    "        #creating directory artifacts/model_training\n",
    "        create_directory([config.root_dir_path])\n",
    "\n",
    "        #creating an object of class variable and assigning value to parameter and taking rtn as fuctn\n",
    "        model_training_config = ModelTrainingConfig(\n",
    "            root_dir_path = config.root_dir_path,\n",
    "            save_best_model_dirpath=config.save_best_model_dirpath,\n",
    "            all_param=param\n",
    "        )\n",
    "        return model_training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd,numpy as np,sklearn\n",
    "from sklearn.pipeline import Pipeline #this class we used to create pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer #to fill null value\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder,OneHotEncoder\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "\n",
    "\n",
    "#step-5) updating the component file of Datatransformation and initializing the class variable as instance variance\n",
    "class DataTransformation():\n",
    "    def __init__(self,transformconfig:DataTransformationConfig):\n",
    "        self.transformconfig = transformconfig #this value we used in our datatransformation stage mei\n",
    "\n",
    "    def get_data_transformation(self):\n",
    "        #In this file we create preprocessor object which is futhure used to transformation\n",
    "        df = pd.read_csv(os.path.join(self.transformconfig.csv_dir_path,\"WineQT.csv\"))\n",
    "\n",
    "        # Extract target column name\n",
    "        target_column_name = list(self.transformconfig.target_column.keys())[0]\n",
    "\n",
    "        #selecting input and output variable\n",
    "        x = df.drop(target_column_name,axis=1)\n",
    "        y = df[target_column_name]\n",
    "\n",
    "        #selecting object and numeric column from input variable\n",
    "        num_feature_lst = x.select_dtypes(exclude='object').columns.to_list()\n",
    "        cat_feature_lst = x.select_dtypes(include='object').columns.to_list()\n",
    "\n",
    "        logger.info(f\"Numeric column from input feature\\n%s\",num_feature_lst)\n",
    "        logger.info(f\"Categorical column from input feature\\n%s\",cat_feature_lst)\n",
    "\n",
    "        #creating numeric pipeline by using Pipeline class\n",
    "        numeric_pipeline = Pipeline(steps=[\n",
    "            (\"imputer\",SimpleImputer(strategy=\"median\")),#filling the numeric featuere will median\n",
    "            (\"scaling\",StandardScaler(with_mean=False))\n",
    "        ])\n",
    "        logger.info(f\"Numeric Pipeline feature\\n%s\",numeric_pipeline)\n",
    "\n",
    "        #creating categorical pipeline by using Pipeline class\n",
    "        categorical_pipeline = Pipeline(steps=[\n",
    "            (\"imputer\",SimpleImputer(strategy=\"most_frequent\")),#filling the categorical feature\n",
    "            (\"onehot\",OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ])\n",
    "        logger.info(f\"Categorical Pipeline feature\\n%s\",categorical_pipeline)\n",
    "\n",
    "        #combining both pipelien using columntransformer class\n",
    "        preprocessor = ColumnTransformer(transformers=[\n",
    "            (\"num_pipeline\",numeric_pipeline,num_feature_lst),\n",
    "            (\"cat_pipeline\",categorical_pipeline,cat_feature_lst)\n",
    "        ])\n",
    "\n",
    "        # #now saving the object into artifacts folder\n",
    "        # save_object(file=self.transformconfig.save_obj_dirpath,obj=preprocessor)\n",
    "\n",
    "        return preprocessor\n",
    "\n",
    "    def initiate_data_transformation(self):\n",
    "        logger.info('Reading train and test Data Using Pandas Library')\n",
    "        train_data = pd.read_csv(os.path.join(self.transformconfig.csv_dir_path,\"train.csv\"))\n",
    "        test_data = pd.read_csv(os.path.join(self.transformconfig.csv_dir_path,\"train.csv\"))\n",
    "\n",
    "        # Extract target column name\n",
    "        target_column_name = list(self.transformconfig.target_column.keys())[0]\n",
    "\n",
    "        #selecting input and output variable from both train,test df object\n",
    "        train_input_feature = train_data.drop(target_column_name,axis=1)\n",
    "        train_output_feature = train_data[target_column_name]\n",
    "\n",
    "        test_input_feature = test_data.drop(target_column_name,axis=1)\n",
    "        test_output_feature = test_data[target_column_name]\n",
    "\n",
    "        #calling the preprocessor object\n",
    "        preprocessor_obj = self.get_data_transformation()\n",
    "\n",
    "        #now saving the object into artifacts folder\n",
    "        save_object(file=Path(self.transformconfig.save_obj_dirpath),obj=preprocessor_obj)\n",
    "\n",
    "        #applying this preprocessor object to input variable only for both train and test df object\n",
    "        input_feature_train_array  = preprocessor_obj.fit_transform(train_input_feature) #changes to 2d numpy array\n",
    "        input_feature_test_array  = preprocessor_obj.transform(test_input_feature)  #changes to 2d numpy array\n",
    "\n",
    "        logger.info('Combining  input feature train array with train_data_output_feature---->to get train_numpy_array')\n",
    "        train_numpy_array = np.c_[input_feature_train_array,np.array(train_output_feature)]\n",
    "        test_numpy_array = np.c_[input_feature_test_array,np.array(test_output_feature)]\n",
    "\n",
    "        return(\n",
    "            train_numpy_array,\n",
    "            test_numpy_array\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step5) update the component file: In this file we create an object for class varibale\n",
    "#and perform model training task accordingly\n",
    "class ModelTraining():\n",
    "    def __init__(self,modelconfig:ModelTrainingConfig):\n",
    "        self.modelconfig = modelconfig\n",
    "\n",
    "    #create kar raha hu method to perform training of model through grid search cv and get best param from it\n",
    "    def initiate_model_training(self,train_numpy_array,test_numpy_array):\n",
    "\n",
    "        #now selecting training and testing data from numpy array obj\n",
    "        x_train, y_train = train_numpy_array[:,:-1],train_numpy_array[:,-1]\n",
    "        x_test, y_test = test_numpy_array[:,:-1],test_numpy_array[:,-1]\n",
    "\n",
    "        #now defining model which i was using for this dataset\n",
    "        models = {\n",
    "            \"ElasticNet\": ElasticNet(),\n",
    "            \"DecisionTreeRegressor\": DecisionTreeRegressor(),\n",
    "            \"RandomForestRegressor\": RandomForestRegressor(),\n",
    "            \"GradientBoostingRegressor\": GradientBoostingRegressor(),\n",
    "            \"SVR\":SVR(),\n",
    "            \"XGBRegressor\":XGBRegressor()\n",
    "        }\n",
    "        \n",
    "        #saving the best models file in dictatonary object\n",
    "        model_report = {}\n",
    "\n",
    "        #using items built in method of dict object to get key and value from it!!!\n",
    "        for model_name,model in models.items():\n",
    "            logger.info(f'Training model: {model_name}')\n",
    "\n",
    "            #Using Grid search CV to find out best parameter and model for this dataset \n",
    "            #creating an object of  Grid search CV class\n",
    "            grid_search_cv = GridSearchCV(\n",
    "                estimator=model,\n",
    "                param_grid=self.modelconfig.all_param.get(model_name, {}),\n",
    "                scoring=\"neg_mean_squared_error\",  # or \"r2\" for R² score\n",
    "                n_jobs=-1,\n",
    "                cv=5,\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            #now gridsearch cv fit the model and findout best hyperpaprameters\n",
    "            grid_search_cv.fit(x_train, y_train)\n",
    "\n",
    "            #after training done getting best model and score from it\n",
    "            logger.info(f\"Best model {model_name} and Best hyperpaprameter Value is {grid_search_cv.best_params_}\")\n",
    "            logger.info(f\"Best model Score {model_name} is : {grid_search_cv.best_score_}\")\n",
    "\n",
    "\n",
    "            #now setting the best hyperparameter value to this model\n",
    "            model.set_params(**grid_search_cv.best_params_)\n",
    "\n",
    "            #now training the model by 80% training data\n",
    "            model.fit(x_train, y_train)\n",
    "\n",
    "            #predicting the output variable using 20% test data\n",
    "            y_pred = model.predict(x_test)\n",
    "\n",
    "            #evaluating the accuracy of model saving them into dict obj\n",
    "            from sklearn.metrics import mean_squared_error, r2_score\n",
    "            model_report[model_name] = {\n",
    "                \"r2\": float(r2_score(y_test, y_pred)),\n",
    "                \"mse\": float(mean_squared_error(y_test,y_pred)),\n",
    "                \"rmse\": float(np.sqrt(mean_squared_error(y_test,y_pred)))\n",
    "            }\n",
    "\n",
    "            print(model_report)\n",
    "\n",
    "            # Get the best model based on R² score\n",
    "            best_model_name = max(model_report, key=lambda x: model_report[x][\"r2\"])\n",
    "            best_model_score = model_report[best_model_name][\"r2\"]\n",
    "            \n",
    "            #now passing the condition if best_model_score >0.85 then only show\n",
    "            if best_model_score > 0.80:\n",
    "                logger.info(f\"Best model is {best_model_name} and best score is {best_model_score}\")\n",
    "                #now saving the object into artifacts folder\n",
    "                save_object(\n",
    "                    file=Path(self.modelcobnfig.save_best_model_dirpath),\n",
    "                    obj = models[best_model_name]\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                logger.info('No Best Score Found')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step6) update the pipeline file\n",
    "try:\n",
    "    cm = ConfigurationManager() #object of configuration manager class\n",
    "    data_transform_config = cm.get_data_transformation_config()\n",
    "    model_config = cm.get_model_training_config()\n",
    "\n",
    "    #creating an object of datatransformation class\n",
    "    dt = DataTransformation(transformconfig=data_transform_config)\n",
    "\n",
    "    dt.get_data_transformation()\n",
    "\n",
    "    train_array,test_array = dt.initiate_data_transformation()\n",
    "\n",
    "    #creating an object of ModelTraining clas\n",
    "    mt = ModelTraining(model_config)\n",
    "    mt.initiate_model_training(\n",
    "        train_numpy_array=train_array,\n",
    "        test_numpy_array=test_array,\n",
    "    )\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    raise CustomException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Wine.Utils import create_directory,read_yaml,download_data_from_s3,save_object\n",
    "from src.Wine.loggers import logger\n",
    "from src.Wine.Exception import CustomException\n",
    "from src.Wine.Constants import *\n",
    "import os,sys\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step3)update the entity file --->is nothing we r defining the class variable\n",
    "#which was used in yaml file and futhure taking rtn as function\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig():\n",
    "    #defining class variable along with dtypes\n",
    "    root_dir_path:Path\n",
    "    save_obj_dirpath: Path\n",
    "    csv_dir_path: Path\n",
    "    target_column:dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating the configurationmanager file in this file we read yaml file and create directories \n",
    "#and assigining values to DataTransformationConfig Class variable \n",
    "\n",
    "class ConfigurationManager():\n",
    "    #creating constructor to initialize the instance variable\n",
    "    def __init__(self,config_filepath = CONFIG_FILEPATH,param_filepath=PARAM_FILEPATH,schema_filepath=SCHEMA_FILEPATH):\n",
    "        self.config = read_yaml(config_filepath) #rtn as configbox dictatonary\n",
    "        self.param =  read_yaml(param_filepath) #rtn as configbox dictionary\n",
    "        self.schema = read_yaml(schema_filepath) #rtn as configbox dictionary\n",
    "\n",
    "        #creating artifacts directory in the project structure\n",
    "        create_directory([self.config.artifacts_root]) #it will create artifacts directory\n",
    "\n",
    "    def get_data_transformation_config(self):\n",
    "        #creating local variable which was used inside this method\n",
    "        transform = self.config.data_transformation\n",
    "        target_coln = self.schema\n",
    "\n",
    "        #creating root directory in artifacts folder for datatransformation\n",
    "        create_directory([transform.root_dir_path]) #create artifacts/data_transformation folder\n",
    "\n",
    "        #creating an object &\n",
    "        #assigining the value to DataTransformationConfig class variable and taking rtn as function\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir_path=transform.root_dir_path,\n",
    "            save_obj_dirpath=transform.save_obj_dirpath,\n",
    "            csv_dir_path=transform.csv_dir_path,\n",
    "            target_column=target_coln.target_column\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd,numpy as np,sklearn\n",
    "from sklearn.pipeline import Pipeline #this class we used to create pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer #to fill null value\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder,OneHotEncoder\n",
    "from sklearn.base import BaseEstimator,TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step-5) updating the component file of Datatransformation and initializing the class variable as instance variance\n",
    "class DataTransformation():\n",
    "    def __init__(self,transformconfig:DataTransformationConfig):\n",
    "        self.transformconfig = transformconfig #this value we used in our datatransformation stage mei\n",
    "\n",
    "    def get_data_transformation(self):\n",
    "        #In this file we create preprocessor object which is futhure used to transformation\n",
    "        df = pd.read_csv(os.path.join(self.transformconfig.csv_dir_path,\"WineQT.csv\"))\n",
    "\n",
    "        # Extract target column name\n",
    "        target_column_name = list(self.transformconfig.target_column.keys())[0]\n",
    "\n",
    "        #selecting input and output variable\n",
    "        x = df.drop(target_column_name,axis=1)\n",
    "        y = df[target_column_name]\n",
    "\n",
    "        #selecting object and numeric column from input variable\n",
    "        num_feature_lst = x.select_dtypes(exclude='object').columns.to_list()\n",
    "        cat_feature_lst = x.select_dtypes(include='object').columns.to_list()\n",
    "\n",
    "        logger.info(f\"Numeric column from input feature\\n%s\",num_feature_lst)\n",
    "        logger.info(f\"Categorical column from input feature\\n%s\",cat_feature_lst)\n",
    "\n",
    "        #creating numeric pipeline by using Pipeline class\n",
    "        numeric_pipeline = Pipeline(steps=[\n",
    "            (\"imputer\",SimpleImputer(strategy=\"median\")),#filling the numeric featuere will median\n",
    "            (\"scaling\",StandardScaler(with_mean=False))\n",
    "        ])\n",
    "        logger.info(f\"Numeric Pipeline feature\\n%s\",numeric_pipeline)\n",
    "\n",
    "        #creating categorical pipeline by using Pipeline class\n",
    "        categorical_pipeline = Pipeline(steps=[\n",
    "            (\"imputer\",SimpleImputer(strategy=\"most_frequent\")),#filling the categorical feature\n",
    "            (\"onehot\",OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ])\n",
    "        logger.info(f\"Categorical Pipeline feature\\n%s\",categorical_pipeline)\n",
    "\n",
    "        #combining both pipelien using columntransformer class\n",
    "        preprocessor = ColumnTransformer(transformers=[\n",
    "            (\"num_pipeline\",numeric_pipeline,num_feature_lst),\n",
    "            (\"cat_pipeline\",categorical_pipeline,cat_feature_lst)\n",
    "        ])\n",
    "\n",
    "        # #now saving the object into artifacts folder\n",
    "        # save_object(file=self.transformconfig.save_obj_dirpath,obj=preprocessor)\n",
    "\n",
    "        return preprocessor\n",
    "\n",
    "    def initiate_data_transformation(self):\n",
    "        logger.info('Reading train and test Data Using Pandas Library')\n",
    "        train_data = pd.read_csv(os.path.join(self.transformconfig.csv_dir_path,\"train.csv\"))\n",
    "        test_data = pd.read_csv(os.path.join(self.transformconfig.csv_dir_path,\"train.csv\"))\n",
    "\n",
    "        # Extract target column name\n",
    "        target_column_name = list(self.transformconfig.target_column.keys())[0]\n",
    "\n",
    "        #selecting input and output variable from both train,test df object\n",
    "        train_input_feature = train_data.drop(target_column_name,axis=1)\n",
    "        train_output_feature = train_data[target_column_name]\n",
    "\n",
    "        test_input_feature = test_data.drop(target_column_name,axis=1)\n",
    "        test_output_feature = test_data[target_column_name]\n",
    "\n",
    "        #calling the preprocessor object\n",
    "        preprocessor_obj = self.get_data_transformation()\n",
    "\n",
    "        #now saving the object into artifacts folder\n",
    "        save_object(file=Path(self.transformconfig.save_obj_dirpath),obj=preprocessor_obj)\n",
    "\n",
    "        #applying this preprocessor object to input variable only for both train and test df object\n",
    "        input_feature_train_array  = preprocessor_obj.fit_transform(train_input_feature) #changes to 2d numpy array\n",
    "        input_feature_test_array  = preprocessor_obj.transform(test_input_feature)  #changes to 2d numpy array\n",
    "\n",
    "        logger.info('Combining  input feature train array with train_data_output_feature---->to get train_numpy_array')\n",
    "        train_numpy_array = np.c_[input_feature_train_array,np.array(train_output_feature)]\n",
    "        test_numpy_array = np.c_[input_feature_test_array,np.array(test_output_feature)]\n",
    "\n",
    "        return(\n",
    "            train_numpy_array,\n",
    "            test_numpy_array\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Wine_ML_AlGO\\\\WineQualityModel'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-30 12:09:53,102]-33-Reading yaml file config\\config.yaml\n",
      "[2024-09-30 12:09:53,148]-33-Reading yaml file param.yaml\n",
      "[2024-09-30 12:09:53,150]-33-Reading yaml file schema.yaml\n",
      "[2024-09-30 12:09:53,174]-46-Creating Directory\n",
      "[2024-09-30 12:09:53,176]-46-Creating Directory\n",
      "[2024-09-30 12:09:53,187]-50-Directory artifacts/data_transformation created\n",
      "[2024-09-30 12:09:53,533]-21-Numeric column from input feature\n",
      "['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'Id']\n",
      "[2024-09-30 12:09:53,535]-22-Categorical column from input feature\n",
      "[]\n",
      "[2024-09-30 12:09:53,537]-29-Numeric Pipeline feature\n",
      "Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
      "                ('scaling', StandardScaler(with_mean=False))])\n",
      "[2024-09-30 12:09:53,543]-36-Categorical Pipeline feature\n",
      "Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
      "[2024-09-30 12:09:53,552]-50-Reading train and test Data Using Pandas Library\n",
      "[2024-09-30 12:09:53,610]-21-Numeric column from input feature\n",
      "['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'Id']\n",
      "[2024-09-30 12:09:53,612]-22-Categorical column from input feature\n",
      "[]\n",
      "[2024-09-30 12:09:53,614]-29-Numeric Pipeline feature\n",
      "Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
      "                ('scaling', StandardScaler(with_mean=False))])\n",
      "[2024-09-30 12:09:53,621]-36-Categorical Pipeline feature\n",
      "Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
      "[2024-09-30 12:09:53,626]-71-Saving Object\n",
      "[2024-09-30 12:09:53,630]-74-Object Saved\n",
      "[2024-09-30 12:09:53,714]-74-Combining  input feature train array with train_data_output_feature---->to get train_numpy_array\n"
     ]
    }
   ],
   "source": [
    "#step6)updating the training pipeline\n",
    "try:\n",
    "    #creating an object of configuration manager\n",
    "    cm = ConfigurationManager()\n",
    "    data_transform_config = cm.get_data_transformation_config()\n",
    "\n",
    "    #creating an object of datatransformation class\n",
    "    dt = DataTransformation(transformconfig=data_transform_config)\n",
    "\n",
    "    dt.get_data_transformation()\n",
    "\n",
    "    train_array,test_array = dt.initiate_data_transformation()\n",
    "    \n",
    "\n",
    "except Exception as e:\n",
    "    raise CustomException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
